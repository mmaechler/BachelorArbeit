\chapter{The {\tt norMmix} Package}


\section{Introduction to the Package}

% it 1
For this thesis, an \Rp package was developed that implements the algorithm
that fits multivariate normal mixtures to given data.
There is a lot of unused code still in the package. These were at one point
implemented used and discarded. They are still included for demonstration.
The {\tt norMmix} package is constructed around the {\tt norMmix} object, that 
codifies a {\tt nor}mal {\tt M}ultivariate {\tt mix}ture model,  and the {\tt 
llnorMmix()} function, that calculates the log-likelihood given a model and 
data.


%% table of params
%% TODO: description of table
\begin{table}
    \centering
    \begin{tabular}{l l}
        \hline
        In Notation & In Code \\
        \hline
        $\pi_i$     & {\tt w, weights} \\
        $\Sigma$    & {\tt Sigma} \\
        $\mu$       & {\tt mu} \\
        $K$         & {\tt k} \\
        dimension   & {\tt p, dim, dims} \\
        components  & {\tt cl, components} \\
        \hline
    \end{tabular}
    \caption{Translation Table: Mathematical Notation to \Rp Code}
    \label{tab:code-notation}
\end{table}


% it 1
The package contains the following functionality:
\begin{table}
\begin{description}
    \item [norMmix] {\tt norMmix()} is the 'init-method' for 
        {\tt norMmix} objects. There exist {\tt is.norMmix} {\tt rnorMmix} and
        {\tt dnorMmix} functions.
    \item [parametrization] The main functions that handle reparametrization
        of models from and to $\pmb{LDL}^\top$ decomposition are {\tt nMm2par}
        and {\tt par2nMm}, which are inverse to each other.
    \item [MLE] The function {\tt norMmixMLE} marries the main components of 
        this package. It initializes a model and parametrizes it for use with 
        {\tt optim}
    \item [model choice] Using {\tt norMmixMLE}, the function fitnMm allows fitting 
        of multiple models and components. Functions analyzing the output of 
        this are also provided, e.g. {\tt BIC} and {\tt print} methods.
    \item [misc] There are also various methods of generics, like {\tt logLik,
        print, BIC, AIC} and {\tt nobs} as well as various {\tt print} methods.
    \item [example objects] Following the paper of \cite{Mar92} various example
        objects are provided and used for study. They follow the naming 
        convention: MW + dimension + number. for example {\tt MW213} for the 
        13-th model of dimension 2.
    \item [simulations] The purpose of this package is to study simulations. 
        there are functions provided to study large collections of evaluated 
        data. e.g {\tt epfl} %TODO: describe and give ref.
\end{description}
\end{table}

relies on {\tt optim()} generic optimizer. maximizes llnormix by varying model 
parameters.

since mclust is one of the more popular packages implementing the EM algo, we 
employ a lot of functions from mclust, to keep things around EM as similar as 
possible.

% it 1
Conceptually, at the start of a fitting algo, e.g. EM we need to initialize a
mixture object. % TODO we (will?) have translation methods to and from mclust
thereafter the paths diverge. at the heart of norMmix's functionality
lie the functions: llnorMmix and nMm2par which are in turn employed by 
norMmixMLE to funnel a mixture object into optim and give optim a function
to optimize.

also relies on {\tt mixtools} package for random generating function 
{\tt rnorMmix} using {\tt rmvnorm}.

\section{On The Development of {\tt norMmix}}

about Cholesky decomp as ldlt. has advantages: fast, parametrically 
parsimonious, can easily compute loglikelihood

%it 1

maybe reread section in McLachlan about accelerating EM algo

not possible to sensibly compare normal mixtures except maybe a strange sorting 
algorithm using Mahalanobis distance or Kullback-Leibler distance or similar
(Hellinger), but not numerically sensible to integrate over potentially 
high-dimensional spaces.

%% TODO: explain comparison


general list of (not necessarily mathematical) dead-ends in the development 
life of the norMmix package.
argue why this is in this section?? because, as a BScT, the learning is as much
part of the research as the results.


% it 1
One dead-end was the parametrization of the weights of a mixture using the 
{\tt logit} function.

<<logit,results=hide>>=
logit <- function(e) {
    stopifnot(is.numeric(e) ,all(e >= 0), all.equal(sum(e),1))
    qlogis(e[-1L])
}

logitinv <- function(e) {
    if (length(e)==0) {return(c(1))}
    stopifnot(is.numeric(e))
    e<- plogis(e)
    sp. <- sum(e)
    w <- c((1-sp.), e)
}
@

This uses the logistical function {\tt logis} to transform to reduce the number
of weights from $K$ to $K-1$. Much like {\tt clr1}, given a list of weights 
{\tt logit} will transform them and {\tt logitinv} will correctly reverse the 
transformation. However, unlike {\tt clr1}, it will not transform an arbitrary 
list of length $K-1$ into a valid weight parameter. For example:

<<logitex,echo=TRUE>>=
w <- runif(7); ret <- logitinv(w)
ret
@

The issue here is that the last line of {\tt logitinv}, which is necessary to 
sum to one, but results in a negative value in {\tt ret[1]} which is not a 
valid weight. The underlying issue is that not every tuple in $\R^{K-1}$ is 
a result of {\tt logit}.

The option to use {\tt logit} is still an argument to {\tt norMmixMLE} by 
specifying {\tt trafo="logit"}, but it shouldn't be used.


% it 1
Another issue during development cropped up during fitting of high dimensional
data. We studied the dataset {\tt SMI.12} from the package {\tt copula}:

<<smi,echo=TRUE>>=
data(SMI.12, package="copula")
str(SMI.12)
@

A consequence of high dimensions is that matrix multiplication is no longer
very stable. As a result, the covariance matrices produced by our own 
implementation of the EM-algorithms m-step ({\tt mstep.nMm}) were not positive
definite.
In the case of {\tt SMI.12}, several covariance matrices are degenerate, which
results in cancellation error with near-zero entries.
We attempted to correct this with the function {\tt forcePositive}, which 
simply tries to set $\pmb{D}$ in $\pmb{LDL}^\top$ greater than zero.
This didn't resolve the issue, since a non-negligible part of the numerical
error was in the $\pmb{L}$ matrix and the resultant covariance matrix was still
not positive definite.

We eventually resolved this issue by abandoning our own implementation and 
using the functions from the {\tt Mclust} package. Not only were these 
numerically stable they were also able to differentiate between models, whereas
ours would assume VVV for every fit.

testing of mvtnorm as proof that ldlt is in fact faster parametrization

mention, that there may be faster ways to apply backsolve. 
quote knuth about premature optimization?

\section{Demonstration}

Mention, that mclust doesn't depend on seed(double check) and therefore norMmix has 
'advantage' of 'confidence intervals'. We can run 50 simulations and see if there
might be more sensible clusters.


% it 1
demonstrate things; essentially put .Rd example sections here
